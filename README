600.120 Intermediate Programming
Section 3 Fall 2016
Final Project

Aleck Zhao azhao9
Noah Halperh nhalper1

Running:
	Our project comes with a makefile in src/ that can be used to make the program. The executable is bin/final. The first command line argument is the path of a file that contains the paths of documents to compare. The second command line argument is optional. Please enter H, M, L, corresponding to high, medium and low sensitivity detection. If the argument is not specified, the program will default to medium sensitivity.

Testing:
	We did not have time to make unit tests for individual classes or methods.

Plan of Attack:
	We decided to make use of the NgramCollection to test similarity of documents. We wanted to see if Ngrams of a certain length could be found in a document to test and any of the rest of the documents to check against. We believe our algorithm is pretty original, but somewhat brute-force.

Design:
	We used the reference implementation of NgramCollection from assignment 8, with some added features, and also eliminated features that wouldn't be useful. We decided to use a class to contain different Ngram collection representations of the text in the file. Our program is run with a class that does all the checking at different sensitivity levels. For the actual detection, we considered a single file at a time, and compared it to every other file in the list (that came afterwards, to eliminate double counting pairs of documents). 

	NgramCollection.cpp: Provides methods to manipulate and create a single Ngram collection for a given N. This uses the reference implementation.

	Document.cpp: Provides methods for creating document objects. A document is a wrapper for a file that stores its path, and generates a 5-gram representation.

	PlagCheck.cpp: Provides methods for detecting plagiarism. The main function is a threshold checker. It compares a document to another, and if there are enough matching Ngrams in both texts, more than the threshold, then that pair of documents is marked as being suspicious.

Flags:
    The flags that can be used as the third element of the input (following the executable and the filename) are "H", "M", and "L". H will run the program with the highest sensitivity, M with medium, and L with low sensitivity. The flags must be entered as capital letters. The different sensitivities all checked 5-grams and thresholds. For high sensitivity, 6 was the threshold. For medium sensitivity, 7 was the threshold. For low sensitivity, 8 was the threshold.

Timing: 
	The program runs relatively quickly on smaller data sets. Approximately 1 second for the small set, and quite fast for medium, but took over 5 minutes for the big data set. We did not even consider  The number of comparisons our program makes is O(n^2) where n is the number of documents in the data set. If we perform direct comparisons, this is as good as we can do. We used a map, which has O(log n) time for finding, so the overall time of the program is roughly O(n^2 log n).

Challenges:
	We found that the convoluted-ness of the data structures we used led to complicated code. It was also difficult to test. We also had difficulty adjusting the parameters, and some of the results were confusing.

Future:
    In the future we would have liked to have written unit tests for the smaller pieces of our program. We also would have researched potential other algorithms that may have been faster, or more refined than the one we came up with. Alternatively, we would have tried to update our program to use std::unordered_map which may have allowed us to search faster, decreasing runtime.
